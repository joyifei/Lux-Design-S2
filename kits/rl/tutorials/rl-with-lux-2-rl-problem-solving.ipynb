{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://www.kaggle.com/code/stonet2000/rl-with-lux-2-rl-problem-solving?scriptVersionId=119773123\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Code\n",
    "\n",
    "Before we start lets install some dependencies. This will also run some extra code that your local notebook may not need to due to how Kaggle Notebooks are setup. **Note that this tutorial is only using the CPU luxai_s2 engine, the jax version will be released later**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --upgrade luxai_s2\n",
    "!pip install --upgrade stable-baselines3==1.5.0\n",
    "!pip install tensorboard\n",
    "!pip install pettingzoo==1.12.0 gym==0.21.0\n",
    "!pip install --upgrade \"importlib_metadata<5.0\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n",
    "__version__ = \"\"\n",
    "# this code above is used for Kaggle Notebooks\n",
    "# You might not need to run this but if you get an attribute error about the gym package, run it"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "import importlib_metadata\n",
    "# kaggle has 6.0.0 installed but we need version <5.0\n",
    "importlib.reload(importlib_metadata)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning for Lux AI Season 2 ðŸ¤–\n",
    "\n",
    "Part 2 of the RL series will now dig into building a working RL agent for the Lux AI Challenge, Season 2!\n",
    "\n",
    "Lux AI is designed to be intuitive to understand, but heavily layered in complexity and interactions of game mechanics in an multi-agent cooperative and competitive environment. \n",
    "\n",
    "Lux AI Season 2's rules can be found here: https://www.lux-ai.org/specs-s2. Make sure to read them to learn how to the game works, and the rest of this tutorial will be much easier to understand.\n",
    "\n",
    "Part 1 of the series covered the single-agent RL setup, but Lux AI Season 2 is multi-agent! Moreover, the environment has different phases and a complex action space which makes it difficult to learn or use of the box. \n",
    "\n",
    "This tutorial will cover simple tools and tricks on how to reduce a complex problem into a easier one! We will primarily focus on three things: \n",
    "\n",
    "1. Simplifying the action space with controllers/action wrappers\n",
    "2. Simplifying observations\n",
    "3. Transforming the three phase Lux AI game into a single phase game\n",
    "\n",
    "Ultimately this will modify the standard RL diagram into one that is \"single-agent\", with modified observations and actions:\n",
    "\n",
    "![](https://github.com/Lux-AI-Challenge/Lux-Design-S2/raw/main/docs/assets/anatomyluxrl.png)\n",
    "\n",
    "\n",
    "This starter kit is also implemented in https://github.com/Lux-AI-Challenge/Lux-Design-S2/tree/main/kits/rl/sb3\n",
    "\n",
    "We highly **recommend running this code with more CPU cores** as RL training can be fairly slow and needs good tuning. A GPU can also speed up the optimization part of RL training, but the rollout/interaction phase is CPU heavy in this tutorial and is typically the bottleneck.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Simplifying the Action Space\n",
    "\n",
    "The action space is quite complicated in Lux S2 as each robot can move, dig, transfer/pickup, all in addition to being able to combine any sequence of these primitives into an action queue of up to length 20. For machine learning, such a massive action space leads to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality), making any ML algorithm have a much harder time to learn something useful, especially in RL.\n",
    "\n",
    "To handle this, we can program a custom Controller that translates actions from one action space to the original action space and adds a few tricks and heuristics to be integrated with RL training. Since the original lux action space is large, this controller can be a little complicated. For those who want to dive straight into training you can use the controller as is. \n",
    "\n",
    "For a high-level overview this controller will\n",
    "- Define a massively simplified action space\n",
    "- Translate actions from the discrete action space into the Lux S2 action space `action_to_lux_action`\n",
    "- Add a heuristic factory action to build one Heavy robot\n",
    "- Generate action masks where False = an action is invalid\n",
    "\n",
    "Overall, the action space of the controller is a discrete action space with just 12 dimensions to control just one heavy robot. It allows for a robot's 4 directional movement, transferring ice in 4 directions in addition to center, picking up power, digging, and a no-op action. This doesn't include factory actions, self destruct, recharging, transferring other types of resources, or longer planned action queues in the action space, which are all open problems for you to potentially tackle!\n",
    "\n",
    "The controller also includes a trick to allow agents to reduce power costs incurred by action queue updates. The controller skips updating action queues if the existing action queue is the same as the new one the agent wants to use for the robot.\n",
    "\n",
    "While this simplification doesn't include adding in more complex things like more heavy robots or planting lichen, it will train out a succesful policy that with simple modifications, will beat the majority of bots using the rule-based starter kits.\n",
    "\n",
    "More advanced usages can consider how to model the actions of different types of units on a game board (e.g. heavy, light, or factory) by using a MultiDiscrete action space. A more practical and likely winning solution can be to use a image-like controller by generating actions for each tile on the board and only using the actions with friendly units on that tile. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for how a image-like controller can work.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "# Controller class copied here since you won't have access to the luxai_s2 package directly on the competition server\n",
    "class Controller:\n",
    "    def __init__(self, action_space: spaces.Space) -> None:\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def action_to_lux_action(\n",
    "        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Takes as input the current \"raw observation\" and the parameterized action and returns\n",
    "        an action formatted for the Lux env\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def action_masks(self, agent: str, obs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Generates a boolean action mask indicating in each discrete dimension whether it would be valid or not\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class SimpleUnitDiscreteController(Controller):\n",
    "    def __init__(self, env_cfg) -> None:\n",
    "        \"\"\"\n",
    "        A simple controller that controls only the robot that will get spawned.\n",
    "        Moreover, it will always try to spawn one heavy robot if there are none regardless of action given\n",
    "\n",
    "        For the robot unit\n",
    "        - 4 cardinal direction movement (4 dims)\n",
    "        - a move center no-op action (1 dim)\n",
    "        - transfer action just for transferring ice in 4 cardinal directions or center (5)\n",
    "        - pickup action for power (1 dims)\n",
    "        - dig action (1 dim)\n",
    "        - no op action (1 dim) - equivalent to not submitting an action queue which costs power\n",
    "\n",
    "        It does not include\n",
    "        - self destruct action\n",
    "        - recharge action\n",
    "        - planning (via actions executing multiple times or repeating actions)\n",
    "        - factory actions\n",
    "        - transferring power or resources other than ice\n",
    "\n",
    "        To help understand how to this controller works to map one action space to the original lux action space,\n",
    "        see how the lux action space is defined in luxai_s2/spaces/action.py\n",
    "\n",
    "        \"\"\"\n",
    "        self.env_cfg = env_cfg\n",
    "        self.move_act_dims = 4\n",
    "        self.transfer_act_dims = 5\n",
    "        self.pickup_act_dims = 1\n",
    "        self.dig_act_dims = 1\n",
    "        self.no_op_dims = 1\n",
    "\n",
    "        self.move_dim_high = self.move_act_dims\n",
    "        self.transfer_dim_high = self.move_dim_high + self.transfer_act_dims\n",
    "        self.pickup_dim_high = self.transfer_dim_high + self.pickup_act_dims\n",
    "        self.dig_dim_high = self.pickup_dim_high + self.dig_act_dims\n",
    "        self.no_op_dim_high = self.dig_dim_high + self.no_op_dims\n",
    "\n",
    "        self.total_act_dims = self.no_op_dim_high\n",
    "        action_space = spaces.Discrete(self.total_act_dims)\n",
    "        super().__init__(action_space)\n",
    "\n",
    "    def _is_move_action(self, id):\n",
    "        return id < self.move_dim_high\n",
    "\n",
    "    def _get_move_action(self, id):\n",
    "        # move direction is id + 1 since we don't allow move center here\n",
    "        return np.array([0, id + 1, 0, 0, 0, 1])\n",
    "\n",
    "    def _is_transfer_action(self, id):\n",
    "        return id < self.transfer_dim_high\n",
    "\n",
    "    def _get_transfer_action(self, id):\n",
    "        id = id - self.move_dim_high\n",
    "        transfer_dir = id % 5\n",
    "        return np.array([1, transfer_dir, 0, self.env_cfg.max_transfer_amount, 0, 1])\n",
    "\n",
    "    def _is_pickup_action(self, id):\n",
    "        return id < self.pickup_dim_high\n",
    "\n",
    "    def _get_pickup_action(self, id):\n",
    "        return np.array([2, 0, 4, self.env_cfg.max_transfer_amount, 0, 1])\n",
    "\n",
    "    def _is_dig_action(self, id):\n",
    "        return id < self.dig_dim_high\n",
    "\n",
    "    def _get_dig_action(self, id):\n",
    "        return np.array([3, 0, 0, 0, 0, 1])\n",
    "\n",
    "    def action_to_lux_action(\n",
    "        self, agent: str, obs: Dict[str, Any], action: npt.NDArray\n",
    "    ):\n",
    "        shared_obs = obs[\"player_0\"]\n",
    "        lux_action = dict()\n",
    "        units = shared_obs[\"units\"][agent]\n",
    "        for unit_id in units.keys():\n",
    "            unit = units[unit_id]\n",
    "            choice = action\n",
    "            action_queue = []\n",
    "            no_op = False\n",
    "            if self._is_move_action(choice):\n",
    "                action_queue = [self._get_move_action(choice)]\n",
    "            elif self._is_transfer_action(choice):\n",
    "                action_queue = [self._get_transfer_action(choice)]\n",
    "            elif self._is_pickup_action(choice):\n",
    "                action_queue = [self._get_pickup_action(choice)]\n",
    "            elif self._is_dig_action(choice):\n",
    "                action_queue = [self._get_dig_action(choice)]\n",
    "            else:\n",
    "                # action is a no_op, so we don't update the action queue\n",
    "                no_op = True\n",
    "\n",
    "            # simple trick to help agents conserve power is to avoid updating the action queue\n",
    "            # if the agent was previously trying to do that particular action already\n",
    "            if len(unit[\"action_queue\"]) > 0 and len(action_queue) > 0:\n",
    "                same_actions = (unit[\"action_queue\"][0] == action_queue[0]).all()\n",
    "                if same_actions:\n",
    "                    no_op = True\n",
    "            if not no_op:\n",
    "                lux_action[unit_id] = action_queue\n",
    "\n",
    "            break\n",
    "\n",
    "        factories = shared_obs[\"factories\"][agent]\n",
    "        if len(units) == 0:\n",
    "            for unit_id in factories.keys():\n",
    "                lux_action[unit_id] = 1  # build a single heavy\n",
    "\n",
    "        return lux_action\n",
    "\n",
    "    def action_masks(self, agent: str, obs: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Defines a simplified action mask for this controller's action space\n",
    "\n",
    "        Doesn't account for whether robot has enough power\n",
    "        \"\"\"\n",
    "\n",
    "        # compute a factory occupancy map that will be useful for checking if a board tile\n",
    "        # has a factory and which team's factory it is.\n",
    "        shared_obs = obs[agent]\n",
    "        factory_occupancy_map = (\n",
    "            np.ones_like(shared_obs[\"board\"][\"rubble\"], dtype=int) * -1\n",
    "        )\n",
    "        factories = dict()\n",
    "        for player in shared_obs[\"factories\"]:\n",
    "            factories[player] = dict()\n",
    "            for unit_id in shared_obs[\"factories\"][player]:\n",
    "                f_data = shared_obs[\"factories\"][player][unit_id]\n",
    "                f_pos = f_data[\"pos\"]\n",
    "                # store in a 3x3 space around the factory position it's strain id.\n",
    "                factory_occupancy_map[\n",
    "                    f_pos[0] - 1 : f_pos[0] + 2, f_pos[1] - 1 : f_pos[1] + 2\n",
    "                ] = f_data[\"strain_id\"]\n",
    "\n",
    "        units = shared_obs[\"units\"][agent]\n",
    "        action_mask = np.zeros((self.total_act_dims), dtype=bool)\n",
    "        for unit_id in units.keys():\n",
    "            action_mask = np.zeros(self.total_act_dims)\n",
    "            # movement is always valid\n",
    "            action_mask[:4] = True\n",
    "\n",
    "            # transferring is valid only if the target exists\n",
    "            unit = units[unit_id]\n",
    "            pos = np.array(unit[\"pos\"])\n",
    "            # a[1] = direction (0 = center, 1 = up, 2 = right, 3 = down, 4 = left)\n",
    "            move_deltas = np.array([[0, 0], [0, -1], [1, 0], [0, 1], [-1, 0]])\n",
    "            for i, move_delta in enumerate(move_deltas):\n",
    "                transfer_pos = np.array(\n",
    "                    [pos[0] + move_delta[0], pos[1] + move_delta[1]]\n",
    "                )\n",
    "                # check if theres a factory tile there\n",
    "                if (\n",
    "                    transfer_pos[0] < 0\n",
    "                    or transfer_pos[1] < 0\n",
    "                    or transfer_pos[0] >= len(factory_occupancy_map)\n",
    "                    or transfer_pos[1] >= len(factory_occupancy_map[0])\n",
    "                ):\n",
    "                    continue\n",
    "                factory_there = factory_occupancy_map[transfer_pos[0], transfer_pos[1]]\n",
    "                if factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]:\n",
    "                    action_mask[\n",
    "                        self.transfer_dim_high - self.transfer_act_dims + i\n",
    "                    ] = True\n",
    "\n",
    "            factory_there = factory_occupancy_map[pos[0], pos[1]]\n",
    "            on_top_of_factory = (\n",
    "                factory_there in shared_obs[\"teams\"][agent][\"factory_strains\"]\n",
    "            )\n",
    "\n",
    "            # dig is valid only if on top of tile with rubble or resources or lichen\n",
    "            board_sum = (\n",
    "                shared_obs[\"board\"][\"ice\"][pos[0], pos[1]]\n",
    "                + shared_obs[\"board\"][\"ore\"][pos[0], pos[1]]\n",
    "                + shared_obs[\"board\"][\"rubble\"][pos[0], pos[1]]\n",
    "                + shared_obs[\"board\"][\"lichen\"][pos[0], pos[1]]\n",
    "            )\n",
    "            if board_sum > 0 and not on_top_of_factory:\n",
    "                action_mask[\n",
    "                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n",
    "                ] = True\n",
    "\n",
    "            # pickup is valid only if on top of factory tile\n",
    "            if on_top_of_factory:\n",
    "                action_mask[\n",
    "                    self.pickup_dim_high - self.pickup_act_dims : self.pickup_dim_high\n",
    "                ] = True\n",
    "                action_mask[\n",
    "                    self.dig_dim_high - self.dig_act_dims : self.dig_dim_high\n",
    "                ] = False\n",
    "\n",
    "            # no-op is always valid\n",
    "            action_mask[-1] = True\n",
    "            break\n",
    "        return action_mask\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Simplifying the Observation Space\n",
    "\n",
    "Lux S2 is fully observable which means you can see everything on the map, the opponents units etc. However, this is very high dimensional and not necessarily easy to learn from due to the curse of dimensionality (again!). We want to simplify this observation space in a way that contains sufficient information to learn a good policy but is also easy to learn from.\n",
    "\n",
    "For this tutorial, we will create a state-based observation space (no image like features e.g. the rubble, ice, ore maps) with some feature engineering that includes useful information such as the distance to the closest factory and ice tile. The wrapper we provide below will use the `gym.ObservationWrapper` interface. Note that since we are focusing on just controlling one heavy robot, the observation wrapper is written to only support one heavy robot (and returns 0 if there are none).\n",
    "\n",
    "\n",
    "More advanced solutions can look into using the full set of observations and designing the appropriate neural net architecture to process them. One idea would be to use convolutional neural networks to process board features like images. See [Season 1's solution by ToadBrigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) and our previous [research paper: Emergent Collective Intelligence from Massive-Agent Cooperation and Competition](https://arxiv.org/abs/2301.01609) for example architectures and feature engineering choices.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class SimpleUnitObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    A simple state based observation to work with in pair with the SimpleUnitDiscreteController\n",
    "\n",
    "    It contains info only on the first robot, the first factory you own, and some useful features. If there are no owned robots the observation is just zero.\n",
    "    No information about the opponent is included. This will generate observations for all teams.\n",
    "\n",
    "    Included features:\n",
    "    - First robot's stats\n",
    "    - distance vector to closest ice tile\n",
    "    - distance vector to first factory\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        super().__init__(env)\n",
    "        self.observation_space = spaces.Box(-999, 999, shape=(13,))\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return SimpleUnitObservationWrapper.convert_obs(obs, self.env.state.env_cfg)\n",
    "\n",
    "    # we make this method static so the submission/evaluation code can use this as well\n",
    "    @staticmethod\n",
    "    def convert_obs(obs: Dict[str, Any], env_cfg: Any) -> Dict[str, npt.NDArray]:\n",
    "        observation = dict()\n",
    "        shared_obs = obs[\"player_0\"]\n",
    "        ice_map = shared_obs[\"board\"][\"ice\"]\n",
    "        ice_tile_locations = np.argwhere(ice_map == 1)\n",
    "\n",
    "        for agent in obs.keys():\n",
    "            obs_vec = np.zeros(\n",
    "                13,\n",
    "            )\n",
    "\n",
    "            factories = shared_obs[\"factories\"][agent]\n",
    "            factory_vec = np.zeros(2)\n",
    "            for k in factories.keys():\n",
    "                # here we track a normalized position of the first friendly factory\n",
    "                factory = factories[k]\n",
    "                factory_vec = np.array(factory[\"pos\"]) / env_cfg.map_size\n",
    "                break\n",
    "            units = shared_obs[\"units\"][agent]\n",
    "            for k in units.keys():\n",
    "                unit = units[k]\n",
    "\n",
    "                # store cargo+power values scaled to [0, 1]\n",
    "                cargo_space = env_cfg.ROBOTS[unit[\"unit_type\"]].CARGO_SPACE\n",
    "                battery_cap = env_cfg.ROBOTS[unit[\"unit_type\"]].BATTERY_CAPACITY\n",
    "                cargo_vec = np.array(\n",
    "                    [\n",
    "                        unit[\"power\"] / battery_cap,\n",
    "                        unit[\"cargo\"][\"ice\"] / cargo_space,\n",
    "                        unit[\"cargo\"][\"ore\"] / cargo_space,\n",
    "                        unit[\"cargo\"][\"water\"] / cargo_space,\n",
    "                        unit[\"cargo\"][\"metal\"] / cargo_space,\n",
    "                    ]\n",
    "                )\n",
    "                unit_type = (\n",
    "                    0 if unit[\"unit_type\"] == \"LIGHT\" else 1\n",
    "                )  # note that build actions use 0 to encode Light\n",
    "                # normalize the unit position\n",
    "                pos = np.array(unit[\"pos\"]) / env_cfg.map_size\n",
    "                unit_vec = np.concatenate(\n",
    "                    [pos, [unit_type], cargo_vec, [unit[\"team_id\"]]], axis=-1\n",
    "                )\n",
    "\n",
    "                # we add some engineered features down here\n",
    "                # compute closest ice tile\n",
    "                ice_tile_distances = np.mean(\n",
    "                    (ice_tile_locations - np.array(unit[\"pos\"])) ** 2, 1\n",
    "                )\n",
    "                # normalize the ice tile location\n",
    "                closest_ice_tile = (\n",
    "                    ice_tile_locations[np.argmin(ice_tile_distances)] / env_cfg.map_size\n",
    "                )\n",
    "                obs_vec = np.concatenate(\n",
    "                    [unit_vec, factory_vec - pos, closest_ice_tile - pos], axis=-1\n",
    "                )\n",
    "                break\n",
    "            observation[agent] = obs_vec\n",
    "\n",
    "        return observation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Transforming Lux S2 into a Single Phase\n",
    "\n",
    "Normally RL frameworks like Stable Baselines 3, RLlib, Tianshou etc. expect the action space and observation space to be consistent throughout an episode. Lux S2 does not conform to this as we add some additional complexity like bidding and factory placement phases. A simple way to get around this is to **upgrade the reset function.**\n",
    "\n",
    "Previously we saw that `env.reset()` resets an environment to a clean slate. We will upgrade this function by building a environment wrapper that not only resets to the clean slate, but also handles the bidding and factory placement phases so effectively agents that are learning start from game states with factories already placed.\n",
    "\n",
    "Below will build a wrapper that works with the SB3 package. To do this, we want to provide the wrapper a bidding policy and factory placement policy which will be used by all teams to handle the first two phases in the reset function. The code below does just that by overriding the environment's reset function in the wrapper. \n",
    "\n",
    "Furthermore, we want to use the Controller we defined earlier, so that is also an argument to the SB3Wrapper and we use it to transform actions inside the `env.step` function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from gym import spaces\n",
    "\n",
    "import luxai_s2.env\n",
    "from luxai_s2.env import LuxAI_S2\n",
    "from luxai_s2.state import ObservationStateDict\n",
    "from luxai_s2.unit import ActionType, BidActionType, FactoryPlacementActionType\n",
    "from luxai_s2.utils import my_turn_to_place_factory\n",
    "from luxai_s2.wrappers.controllers import (\n",
    "    Controller,\n",
    ")\n",
    "\n",
    "\n",
    "class SB3Wrapper(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: LuxAI_S2,\n",
    "        bid_policy: Callable[\n",
    "            [str, ObservationStateDict], Dict[str, BidActionType]\n",
    "        ] = None,\n",
    "        factory_placement_policy: Callable[\n",
    "            [str, ObservationStateDict], Dict[str, FactoryPlacementActionType]\n",
    "        ] = None,\n",
    "        controller: Controller = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        A environment wrapper for Stable Baselines 3. It reduces the LuxAI_S2 env\n",
    "        into a single phase game and places the first two phases (bidding and factory placement) into the env.reset function so that\n",
    "        interacting agents directly start generating actions to play the third phase of the game.\n",
    "\n",
    "        It also accepts a Controller that translates action's in one action space to a Lux S2 compatible action\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        bid_policy: Function\n",
    "            A function accepting player: str and obs: ObservationStateDict as input that returns a bid action\n",
    "            such as dict(bid=10, faction=\"AlphaStrike\"). By default will bid 0\n",
    "        factory_placement_policy: Function\n",
    "            A function accepting player: str and obs: ObservationStateDict as input that returns a factory placement action\n",
    "            such as dict(spawn=np.array([2, 4]), metal=150, water=150). By default will spawn in a random valid location with metal=150, water=150\n",
    "        controller : Controller\n",
    "            A controller that parameterizes the action space into something more usable and converts parameterized actions to lux actions.\n",
    "            See luxai_s2/wrappers/controllers.py for available controllers and how to make your own\n",
    "        \"\"\"\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.env = env\n",
    "        \n",
    "        assert controller is not None\n",
    "        \n",
    "        # set our controller and replace the action space\n",
    "        self.controller = controller\n",
    "        self.action_space = controller.action_space\n",
    "\n",
    "        # The simplified wrapper removes the first two phases of the game by using predefined policies (trained or heuristic)\n",
    "        # to handle those two phases during each reset\n",
    "        if factory_placement_policy is None:\n",
    "            def factory_placement_policy(player, obs: ObservationStateDict):\n",
    "                potential_spawns = np.array(\n",
    "                    list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n",
    "                )\n",
    "                spawn_loc = potential_spawns[\n",
    "                    np.random.randint(0, len(potential_spawns))\n",
    "                ]\n",
    "                return dict(spawn=spawn_loc, metal=150, water=150)\n",
    "\n",
    "        self.factory_placement_policy = factory_placement_policy\n",
    "        if bid_policy is None:\n",
    "            def bid_policy(player, obs: ObservationStateDict):\n",
    "                faction = \"AlphaStrike\"\n",
    "                if player == \"player_1\":\n",
    "                    faction = \"MotherMars\"\n",
    "                return dict(bid=0, faction=faction)\n",
    "\n",
    "        self.bid_policy = bid_policy\n",
    "\n",
    "        self.prev_obs = None\n",
    "\n",
    "    def step(self, action: Dict[str, npt.NDArray]):\n",
    "        \n",
    "        # here, for each agent in the game we translate their action into a Lux S2 action\n",
    "        lux_action = dict()\n",
    "        for agent in self.env.agents:\n",
    "            if agent in action:\n",
    "                lux_action[agent] = self.controller.action_to_lux_action(\n",
    "                    agent=agent, obs=self.prev_obs, action=action[agent]\n",
    "                )\n",
    "            else:\n",
    "                lux_action[agent] = dict()\n",
    "        \n",
    "        # lux_action is now a dict mapping agent name to an action\n",
    "        obs, reward, done, info = self.env.step(lux_action)\n",
    "        self.prev_obs = obs\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        # we upgrade the reset function here\n",
    "        \n",
    "        # we call the original reset function first\n",
    "        obs = self.env.reset(**kwargs)\n",
    "        \n",
    "        # then use the bid policy to go through the bidding phase\n",
    "        action = dict()\n",
    "        for agent in self.env.agents:\n",
    "            action[agent] = self.bid_policy(agent, obs[agent])\n",
    "        obs, _, _, _ = self.env.step(action)\n",
    "        \n",
    "        # while real_env_steps < 0, we are in the factory placement phase\n",
    "        # so we use the factory placement policy to step through this\n",
    "        while self.env.state.real_env_steps < 0:\n",
    "            action = dict()\n",
    "            for agent in self.env.agents:\n",
    "                if my_turn_to_place_factory(\n",
    "                    obs[\"player_0\"][\"teams\"][agent][\"place_first\"],\n",
    "                    self.env.state.env_steps,\n",
    "                ):\n",
    "                    action[agent] = self.factory_placement_policy(agent, obs[agent])\n",
    "                else:\n",
    "                    action[agent] = dict()\n",
    "            obs, _, _, _ = self.env.step(action)\n",
    "        self.prev_obs = obs\n",
    "        \n",
    "        return obs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining a Bid and Factory Placement policy\n",
    "\n",
    "To test the code above, we can program some heuristic bid and factory placement policies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def zero_bid(player, obs):\n",
    "    # a policy that always bids 0\n",
    "    faction = \"AlphaStrike\"\n",
    "    if player == \"player_1\":\n",
    "        faction = \"MotherMars\"\n",
    "    return dict(bid=0, faction=faction)\n",
    "\n",
    "def place_near_random_ice(player, obs):\n",
    "    \"\"\"\n",
    "    This policy will place a single factory with all the starting resources\n",
    "    near a random ice tile\n",
    "    \"\"\"\n",
    "    if obs[\"teams\"][player][\"metal\"] == 0:\n",
    "        return dict()\n",
    "    potential_spawns = list(zip(*np.where(obs[\"board\"][\"valid_spawns_mask\"] == 1)))\n",
    "    potential_spawns_set = set(potential_spawns)\n",
    "    done_search = False\n",
    "    \n",
    "    # simple numpy trick to find locations adjacent to ice tiles.\n",
    "    ice_diff = np.diff(obs[\"board\"][\"ice\"])\n",
    "    pot_ice_spots = np.argwhere(ice_diff == 1)\n",
    "    if len(pot_ice_spots) == 0:\n",
    "        pot_ice_spots = potential_spawns\n",
    "    \n",
    "    # pick a random ice spot and search around it for spawnable locations.\n",
    "    trials = 5\n",
    "    while trials > 0:\n",
    "        pos_idx = np.random.randint(0, len(pot_ice_spots))\n",
    "        pos = pot_ice_spots[pos_idx]\n",
    "        area = 3\n",
    "        for x in range(area):\n",
    "            for y in range(area):\n",
    "                check_pos = [pos[0] + x - area // 2, pos[1] + y - area // 2]\n",
    "                if tuple(check_pos) in potential_spawns_set:\n",
    "                    done_search = True\n",
    "                    pos = check_pos\n",
    "                    break\n",
    "            if done_search:\n",
    "                break\n",
    "        if done_search:\n",
    "            break\n",
    "        trials -= 1\n",
    "    \n",
    "    if not done_search:\n",
    "        spawn_loc = potential_spawns[np.random.randint(0, len(potential_spawns))]\n",
    "        pos = spawn_loc\n",
    "    \n",
    "    # this will spawn a factory at pos and with all the starting metal and water\n",
    "    metal = obs[\"teams\"][player][\"metal\"]\n",
    "    return dict(spawn=pos, metal=metal, water=metal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So **without the wrapper**, when we reset the environment it looks like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"LuxAI_S2-v0\")\n",
    "env.reset(seed=0)\n",
    "img = env.render(\"rgb_array\")\n",
    "plt.imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**With the wrapper**, when we reset the environment it looks like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"LuxAI_S2-v0\")\n",
    "env = SB3Wrapper(env, zero_bid, place_near_random_ice, controller=SimpleUnitDiscreteController(env.env_cfg))\n",
    "env.reset(seed=0)\n",
    "img = env.render(\"rgb_array\")\n",
    "plt.imshow(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Success! Our upgraded reset function makes the environment now start from the start of the normal game phase, meaning the action space can be consistently the same throughout the game."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Training with RL\n",
    "\n",
    "In the previous tutorial, we saw how to train an agent with SB3 in single-agent environments. Handling true multi-agent via training separate or shared policies to control all agents requires a few extra things so instead, for the purpose of a tutorial we will treat Lux S2 like a single agent environment by training a policy for one team and letting the other team simply do nothing.\n",
    "\n",
    "Moreover, we want to define our own reward function to encourage our robots to seek ice, dig it, and return to a factory so it can generate water and survive longer. To do this all, we will just create a custom environment wrapper.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import copy\n",
    "class CustomEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        \"\"\"\n",
    "        Adds a custom reward and turns the LuxAI_S2 environment into a single-agent environment for easy training\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.prev_step_metrics = None\n",
    "\n",
    "    def step(self, action):\n",
    "        agent = \"player_0\"\n",
    "        opp_agent = \"player_1\"\n",
    "\n",
    "        opp_factories = self.env.state.factories[opp_agent]\n",
    "        for k in opp_factories.keys():\n",
    "            factory = opp_factories[k]\n",
    "             # set enemy factories to have 1000 water to keep them alive the whole around and treat the game as single-agent\n",
    "            factory.cargo.water = 1000\n",
    "\n",
    "        # submit actions for just one agent to make it single-agent\n",
    "        # and save single-agent versions of the data below\n",
    "        action = {agent: action}\n",
    "        obs, _, done, info = self.env.step(action)\n",
    "        obs = obs[agent]\n",
    "        done = done[agent]\n",
    "        \n",
    "        # we collect stats on teams here. These are useful stats that can be used to help generate reward functions\n",
    "        stats: StatsStateDict = self.env.state.stats[agent]\n",
    "\n",
    "        info = dict()\n",
    "        metrics = dict()\n",
    "        metrics[\"ice_dug\"] = (\n",
    "            stats[\"generation\"][\"ice\"][\"HEAVY\"] + stats[\"generation\"][\"ice\"][\"LIGHT\"]\n",
    "        )\n",
    "        metrics[\"water_produced\"] = stats[\"generation\"][\"water\"]\n",
    "\n",
    "        # we save these two to see often the agent updates robot action queues and how often enough\n",
    "        # power to do so and succeed (less frequent updates = more power is saved)\n",
    "        metrics[\"action_queue_updates_success\"] = stats[\"action_queue_updates_success\"]\n",
    "        metrics[\"action_queue_updates_total\"] = stats[\"action_queue_updates_total\"]\n",
    "\n",
    "        # we can save the metrics to info so we can use tensorboard to log them to get a glimpse into how our agent is behaving\n",
    "        info[\"metrics\"] = metrics\n",
    "\n",
    "        reward = 0\n",
    "        if self.prev_step_metrics is not None:\n",
    "            # we check how much ice and water is produced and reward the agent for generating both\n",
    "            ice_dug_this_step = metrics[\"ice_dug\"] - self.prev_step_metrics[\"ice_dug\"]\n",
    "            water_produced_this_step = (\n",
    "                metrics[\"water_produced\"] - self.prev_step_metrics[\"water_produced\"]\n",
    "            )\n",
    "            # we reward water production more as it is the most important resource for survival\n",
    "            reward = ice_dug_this_step / 100 + water_produced_this_step\n",
    "\n",
    "        self.prev_step_metrics = copy.deepcopy(metrics)\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)[\"player_0\"]\n",
    "        self.prev_step_metrics = None\n",
    "        return obs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Defining the Environment and using Wrappers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will define a `make_env` function and use it with SB3 to create multiple environments in parallel that scale with the number of CPU cores you have. A future tutorial will show a variant that creates a single jax-powered environment to achieve the same functionality but scaling with GPU.\n",
    "\n",
    "We will use the SB3Wrapper, the controller and observation wrapper we defined, and the custom env wrapper as well. These put together will give us an environment that resets to the start of the normal game phase, has a consistent and simplified observation and action space, and contains our reward function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from gym.wrappers import TimeLimit\n",
    "def make_env(env_id: str, rank: int, seed: int = 0, max_episode_steps=200):\n",
    "    def _init() -> gym.Env:\n",
    "        # verbose = 0\n",
    "        # collect_stats=True lets us track stats like total ice dug during an episode to help create reward functions\n",
    "        # max factories set to 2 for simplification and keeping returns consistent as we survive longer \n",
    "        # if there are more initial resources\n",
    "        env = gym.make(env_id, verbose=0, collect_stats=True, MAX_FACTORIES=2)\n",
    "\n",
    "        # Add a SB3 wrapper to make it work with SB3 and simplify the action space with the controller\n",
    "        # this will remove the bidding phase and factory placement phase. For factory placement we use\n",
    "        # the provided place_near_random_ice function which will randomly select an ice tile and place a factory near it.\n",
    "        env = SB3Wrapper(\n",
    "            env,\n",
    "            factory_placement_policy=place_near_random_ice,\n",
    "            controller=SimpleUnitDiscreteController(env.env_cfg),\n",
    "        )\n",
    "        \n",
    "        # changes observation to include a few simple features\n",
    "        env = SimpleUnitObservationWrapper(\n",
    "            env\n",
    "        )\n",
    "        \n",
    "        # convert to single agent, adds our reward\n",
    "        env = CustomEnvWrapper(env)  \n",
    "        \n",
    "        # Add a timelimit to the environment, which can truncate episodes, speed up training\n",
    "        env = TimeLimit(\n",
    "            env, max_episode_steps=max_episode_steps\n",
    "        )\n",
    "        env = Monitor(env) # for SB3 to allow it to record metrics\n",
    "        env.reset(seed=seed + rank)\n",
    "        set_random_seed(seed)\n",
    "        return env\n",
    "\n",
    "    return _init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a7c2f6e1",
   "metadata": {
    "papermill": {
     "duration": 0.015859,
     "end_time": "2023-02-20T17:42:20.203492",
     "exception": false,
     "start_time": "2023-02-20T17:42:20.187633",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next we will define a useful callback function to log some of the custom metrics we defined earlier in the CustomEnvWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b5650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T17:42:20.236637Z",
     "iopub.status.busy": "2023-02-20T17:42:20.235412Z",
     "iopub.status.idle": "2023-02-20T17:42:20.247003Z",
     "shell.execute_reply": "2023-02-20T17:42:20.244903Z"
    },
    "papermill": {
     "duration": 0.030769,
     "end_time": "2023-02-20T17:42:20.249845",
     "exception": false,
     "start_time": "2023-02-20T17:42:20.219076",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback\n",
    "class TensorboardCallback(BaseCallback):\n",
    "    def __init__(self, tag: str, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.tag = tag\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        c = 0\n",
    "\n",
    "        for i, done in enumerate(self.locals[\"dones\"]):\n",
    "            if done:\n",
    "                info = self.locals[\"infos\"][i]\n",
    "                c += 1\n",
    "                for k in info[\"metrics\"]:\n",
    "                    stat = info[\"metrics\"][k]\n",
    "                    self.logger.record_mean(f\"{self.tag}/{k}\", stat)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de19fb3b",
   "metadata": {
    "papermill": {
     "duration": 0.018109,
     "end_time": "2023-02-20T17:42:20.282346",
     "exception": false,
     "start_time": "2023-02-20T17:42:20.264237",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 Training Setup\n",
    "\n",
    "Now we can prepare for training by creating training and evaluation environments, as well as defining our algorithm and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac78e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T17:42:20.320852Z",
     "iopub.status.busy": "2023-02-20T17:42:20.319188Z",
     "iopub.status.idle": "2023-02-20T17:42:28.609995Z",
     "shell.execute_reply": "2023-02-20T17:42:28.608291Z"
    },
    "papermill": {
     "duration": 8.315235,
     "end_time": "2023-02-20T17:42:28.612523",
     "exception": false,
     "start_time": "2023-02-20T17:42:20.297288",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.ppo import PPO\n",
    "\n",
    "set_random_seed(42)\n",
    "log_path = \"logs/exp_1\"\n",
    "num_envs = 4\n",
    "\n",
    "# set max episode steps to 200 for training environments to train faster\n",
    "env = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=200) for i in range(num_envs)])\n",
    "env.reset()\n",
    "# set max episode steps to 1000 to match original environment\n",
    "eval_env = SubprocVecEnv([make_env(\"LuxAI_S2-v0\", i, max_episode_steps=1000) for i in range(4)])\n",
    "eval_env.reset()\n",
    "rollout_steps = 4000\n",
    "policy_kwargs = dict(net_arch=(128, 128))\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    n_steps=rollout_steps // num_envs,\n",
    "    batch_size=800,\n",
    "    learning_rate=3e-4,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    n_epochs=2,\n",
    "    target_kl=0.05,\n",
    "    gamma=0.99,\n",
    "    tensorboard_log=osp.join(log_path),\n",
    ")\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=osp.join(log_path, \"models\"),\n",
    "    log_path=osp.join(log_path, \"eval_logs\"),\n",
    "    eval_freq=24_000,\n",
    "    deterministic=False,\n",
    "    render=False,\n",
    "    n_eval_episodes=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef97f99",
   "metadata": {
    "papermill": {
     "duration": 0.013638,
     "end_time": "2023-02-20T17:42:28.64116",
     "exception": false,
     "start_time": "2023-02-20T17:42:28.627522",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With our callback functions and model defined, we can now begin training using `model.learn`. On CPU this training can take around 3-4 hours to train, on GPU it can take 30min to an hour to train. The hyperparameters and reward function can be improved to make it train much faster. A simple way to also increase training speed is to train on a machine with more CPU cores and increasing `num_envs` above. Kaggle notebooks by default only have 4, but with e.g. 10 you can easily train a policy in around 30 minutes.\n",
    "\n",
    "If you want to skip this training you can also just use the pretrained model that's in the downloaded dataset for the RL kit called `best_model.dontunzipme`. (kaggle auto unzips files but we need to keep it as a zip so the file extention is called .dontunzipme but for submission just change it to a .zip)\n",
    "\n",
    "To track the progress we recommend using tensorboard which you can run with\n",
    "```\n",
    "tensorboard --logdir logs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e24412",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T17:42:28.672708Z",
     "iopub.status.busy": "2023-02-20T17:42:28.671974Z",
     "iopub.status.idle": "2023-02-20T21:08:25.698631Z",
     "shell.execute_reply": "2023-02-20T21:08:25.696767Z"
    },
    "papermill": {
     "duration": 12357.04555,
     "end_time": "2023-02-20T21:08:25.701412",
     "exception": false,
     "start_time": "2023-02-20T17:42:28.655862",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total_timesteps = 10_000_000\n",
    "model.learn(\n",
    "    total_timesteps,\n",
    "    callback=[TensorboardCallback(tag=\"train_metrics\"), eval_callback],\n",
    ")\n",
    "model.save(osp.join(log_path, \"models/latest_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4506c8",
   "metadata": {
    "papermill": {
     "duration": 0.257607,
     "end_time": "2023-02-20T21:08:26.20571",
     "exception": false,
     "start_time": "2023-02-20T21:08:25.948103",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Packaging and Submission\n",
    "\n",
    "We now have a trained policy. In order to make it submittable to the competition we recommend you write code on separate files and only use kaggle notebooks for training as it can get very messy to program an RL agent just using a Kaggle notebook interface. The starter kit that was downloaded earlier has all of the code above written already and organized into separate files and folders. The observation wrapper and controller written here are saved to the `wrappers` folder. The SB3Wrapper is not in the kit, but is a part of the official luxai_s2 package and you can import it with\n",
    "\n",
    "```\n",
    "from luxai_s2.wrappers import SB3Wrapper\n",
    "```\n",
    "\n",
    "The main files to take note of are `nn.py` and `agent.py`. Since kaggle servers don't have Stable Baselines 3 installed, `nn.py` is where we program some utility functions as well as the neural network model to load the SB3 trained weights into a PyTorch neural network model. `agent.py` will then use those utilities to load the model zip file at `MODEL_WEIGHTS_RELATIVE_PATH` which can be changed at the top of `agent.py`\n",
    "\n",
    "`agent.py` also uses the actions_mask function to invalidate some actions so that the policy only generates valid actions, which is a easy way to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689bd051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T21:08:26.746226Z",
     "iopub.status.busy": "2023-02-20T21:08:26.7444Z",
     "iopub.status.idle": "2023-02-20T21:08:27.511818Z",
     "shell.execute_reply": "2023-02-20T21:08:27.51005Z"
    },
    "papermill": {
     "duration": 1.050097,
     "end_time": "2023-02-20T21:08:27.514671",
     "exception": false,
     "start_time": "2023-02-20T21:08:26.464574",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if running on kaggle, run below to copy the rl starter kit files to the working directory\n",
    "!cp -r ../input/luxai-s2-rl-sb3-kit/* .\n",
    "!mv best_model.dontunzipme best_model.zip # kaggle auto unzips files but we don't want it to here so we do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09869a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T21:08:28.035127Z",
     "iopub.status.busy": "2023-02-20T21:08:28.034693Z",
     "iopub.status.idle": "2023-02-20T21:08:28.345495Z",
     "shell.execute_reply": "2023-02-20T21:08:28.343299Z"
    },
    "papermill": {
     "duration": 0.578032,
     "end_time": "2023-02-20T21:08:28.348563",
     "exception": false,
     "start_time": "2023-02-20T21:08:27.770531",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if you trained an actual agent, copy its model weights here\n",
    "!mv logs/exp_1/models/best_model.zip best_model.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cffddf",
   "metadata": {
    "papermill": {
     "duration": 0.244234,
     "end_time": "2023-02-20T21:08:28.844464",
     "exception": false,
     "start_time": "2023-02-20T21:08:28.60023",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "To submit your trained agent create a .tar.gz file. You can download the submission.tar.gz file from the right and submit it to the competition directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d5cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-20T21:08:29.3511Z",
     "iopub.status.busy": "2023-02-20T21:08:29.350631Z",
     "iopub.status.idle": "2023-02-20T21:08:29.852735Z",
     "shell.execute_reply": "2023-02-20T21:08:29.851137Z"
    },
    "papermill": {
     "duration": 0.754135,
     "end_time": "2023-02-20T21:08:29.855605",
     "exception": false,
     "start_time": "2023-02-20T21:08:29.10147",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!tar -cvzf submission.tar.gz *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b81a1a",
   "metadata": {
    "papermill": {
     "duration": 0.237095,
     "end_time": "2023-02-20T21:08:30.343678",
     "exception": false,
     "start_time": "2023-02-20T21:08:30.106583",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tips for Improving your Agent\n",
    "\n",
    "This tutorial agent will train a policy that can efficiently control a single heavy robot that learns to pickup power, constantly dig ice, and transfer ice back to the factory and survive the full 1000 turns in the game. A simple improvement would be to add lichen planting to the action space / controller or program it directly as a rule in the agent.py file, allowing you to score points by the end of the game as well as generate more power.\n",
    "\n",
    "Another easy idea is to modify the `agent.py` code so that you spawn multiple factories and multiple heavy robots, and simply run the trained policy on each heavy robot.\n",
    "\n",
    "\n",
    "If you want to look into more scalable solutions, it's critical to first figure out how to model multiple units at once. This kit shows you how to control a single heavy robot effectively but not multiple. Another thing to consider is what observations and features would be the most useful. Finally, you can always try and develop a more complex action controller in addition to developing better reward functions.\n",
    "\n",
    "If you feel you are experienced enough, you can take a look at [last season's winning solution by team Toad Brigade](https://www.kaggle.com/competitions/lux-ai-2021/discussion/294993) or [our paper: Emergent collective intelligence from massive-agent cooperation and competition](https://arxiv.org/abs/2301.01609) which show how to use convolutional neural nets and various other techniques (e.g. invalid action masking) to control a massive number of units at once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12452.19971,
   "end_time": "2023-02-20T21:08:33.226852",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-20T17:41:01.027142",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}